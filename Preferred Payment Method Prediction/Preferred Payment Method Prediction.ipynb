{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "926fac2c-edcc-4966-abd7-58ed129d4a0b",
   "metadata": {},
   "source": [
    "# Preferred Payment Method Prediction\n",
    "\n",
    "\n",
    "### Reg No: IT21134180\n",
    "### Name: Vihansa S.A.S\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<ul>\n",
    "    <li><b>Target Variable:</b> Payment (Cash, Credit card, Ewallet) </li>\n",
    "    <li><b>Predictors:</b> Branch, Customer type, Gender, Product line, Quantity, Date, Time, COGS, and Gross income</li>\n",
    "    <li><b>Objective:</b> Predict the preferred payment method for customers based on their purchase behavior, allowing the business to tailor promotions or offers to encourage the use of specific payment methods.</li>\n",
    "</ul>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b9eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the dataset \n",
    "dataset = pd.read_csv('../dataset/supermarket_sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286ac9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the dataset \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select columns in the dataset \n",
    "dataset = dataset[[\"Branch\",\"Customer type\",\"Gender\",\"Product line\",\"Time\",\"Quantity\",\"cogs\",\"gross income\",\"Payment\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f82f0d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for Missing values \n",
    "dataset.isnull() #It is difficult to identify missing values using this method since the dataset is large.\n",
    "dataset.isnull().values.any() #Using this command we can scan the entire dataset and get a verification whether there are missing values or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the data into numerical values [Label Encoding]\n",
    "\n",
    "#Getting the count of labels available in the relevant columns \n",
    "dataset[\"Branch\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b60aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[:, 'Time'] = pd.to_datetime(dataset['Time'], format='%H:%M')\n",
    "\n",
    "# Convert timestamp column to datetime type\n",
    "dataset['Time'] = pd.to_datetime(dataset['Time'])\n",
    "\n",
    "# Extract relevant time components\n",
    "dataset['year'] = dataset['Time'].dt.year\n",
    "dataset['month'] = dataset['Time'].dt.month\n",
    "dataset['day'] = dataset['Time'].dt.day\n",
    "dataset['hour'] = dataset['Time'].dt.hour\n",
    "dataset['minute'] = dataset['Time'].dt.minute\n",
    "dataset['second'] = dataset['Time'].dt.second\n",
    "\n",
    "# Encode cyclical features\n",
    "dataset['hour_sin'] = np.sin(2 * np.pi * dataset['hour'] / 24)\n",
    "dataset['hour_cos'] = np.cos(2 * np.pi * dataset['hour'] / 24)\n",
    "\n",
    "\n",
    "# Drop original timestamp column\n",
    "dataset.drop('Time', axis=1, inplace=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8460bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas DataFrame from the data\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Get the number of labels (unique categories) for each column\n",
    "num_labels = df.nunique()\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b525e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise label encoder\n",
    "label_encoder = LabelEncoder() \n",
    "  \n",
    "# Encode labels in the data frame. \n",
    "df['Branch']= label_encoder.fit_transform(df['Branch']) \n",
    "df['Customer type']= label_encoder.fit_transform(df['Customer type']) \n",
    "df['Gender']= label_encoder.fit_transform(df['Gender']) \n",
    "df['Product line']= label_encoder.fit_transform(df['Product line']) \n",
    "df['Payment']= label_encoder.fit_transform(df['Payment'])   \n",
    "\n",
    "#Print the data frame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b85a1cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming df['cogs'] contains your data\n",
    "cogs_array = np.array(df['cogs'])  # Convert to numpy array if not already\n",
    "gross_array = np.array(df['gross income'])\n",
    "\n",
    "# Reshape the array to 2D array\n",
    "cogs_array_reshaped = cogs_array.reshape(-1, 1)\n",
    "grossincome_array_reshaped = gross_array.reshape(-1, 1)\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "MinMax = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the reshaped array\n",
    "df['cogs'] = MinMax.fit_transform(cogs_array_reshaped)\n",
    "df['gross income'] = MinMax.fit_transform(grossincome_array_reshaped)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1368079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the data types of the dataset\n",
    "print(dataset.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb0384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Split the dataset as X and Y \n",
    "x = df.drop(['Payment'],axis=1)\n",
    "y = df['Payment'] #value that is expected to predict \n",
    "\n",
    "#Print X and Y \n",
    "print(x)\n",
    "print (y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b78cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train and test dataset \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2,train_size=0.8,random_state=1,shuffle=False)\n",
    "print('x train')\n",
    "print(x_train)\n",
    "\n",
    "print('X test')\n",
    "print (x_test)\n",
    "\n",
    "print('y train')\n",
    "print(y_train)\n",
    "\n",
    "print('y test')\n",
    "print (y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ba397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and parameters\n",
    "grid = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': range(2, 10),\n",
    "    'min_samples_split': [2, 4, 6, 8, 50],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_leaf_nodes': [None, 5, 10, 20],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2],\n",
    "    'ccp_alpha': [0.0, 0.1, 0.2],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy', error_score='raise')\n",
    "grid_result = grid_search.fit(x_train, np.ravel(y_train))  # Ensure y_train is flat (1D)\n",
    "\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8098e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model \n",
    "clf = DecisionTreeClassifier(criterion='log_loss',splitter='random',max_depth=7,min_samples_split=4, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
    "\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85109d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trained model prediction and accuracy \n",
    "prediction = clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test,prediction)\n",
    "accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da12677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train dataset based on Gaussian naive bayes algorithm \n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train, y_train)\n",
    "# making predictions on the testing set\n",
    "y_pred = gnb.predict(x_test)\n",
    "#comparing actual responses with predicted response value \n",
    "print(\"Gaussian Naive Bayes model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda65984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machines : SVC algorithm \n",
    "clf = make_pipeline(StandardScaler(), SVC(C=1.0,gamma='auto',decision_function_shape='ovo'))\n",
    "clf.fit(x_train, y_train)\n",
    "# pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "#                 ('svc', SVC(gamma='auto'))])\n",
    "prediction = clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test,prediction)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda75b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machines : NuSVC algorithm \n",
    "clf = make_pipeline(StandardScaler(), NuSVC(nu=0.5, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None))\n",
    "clf.fit(x_train, y_train)\n",
    "# pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "#                 ('svc', SVC(gamma='auto'))])\n",
    "prediction = clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test,prediction)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Algorithm \n",
    "clf = RandomForestClassifier(n_estimators=10,max_depth=2, random_state=0).fit(x_train, y_train)\n",
    "clf.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f916ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RidgeClassifier \n",
    "clf = RidgeClassifier(alpha=1,copy_X=False,solver='sag').fit(x_train, y_train)\n",
    "clf.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e57f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BaggingClassifier\n",
    "# x_train, y_train = make_classification(n_samples=1000, n_features=7,n_informative=2, n_redundant=0,random_state=0, shuffle=False)\n",
    "clf = BaggingClassifier(estimator=SVC(),n_estimators=10, random_state=0).fit(x_train, y_train)\n",
    "clf.score(x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
